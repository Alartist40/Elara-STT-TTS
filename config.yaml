system:
  name: "Elara Mini"
  version: "0.1.0"
  
model:
  # Options: local | api | ollama | download
  backend: "local"
  
  # LOCAL backend (your current setup)
  local:
    path: "models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
    threads: 4
    context_size: 4096
  
  # API backend (OpenAI, Anthropic, etc)
  api:
    provider: "openai"  # openai | anthropic
    key: ""             # Set via env var ELARA_API_KEY preferably
    model: "gpt-3.5-turbo"
    url: "https://api.openai.com/v1/chat/completions"
  
  # OLLAMA backend (local HTTP server)
  ollama:
    url: "http://localhost:11434/api/generate"
    model: "llama2"
  
  # DOWNLOAD backend (auto-download from HF)
  download:
    repo: "bartowski/gemma-3-1b-it-GGUF"
    file: "gemma-3-1b-it-q4_0.gguf"
    cache_dir: "models"

voice:
  stt_model: "tiny"
  record_seconds: 5
  tts_speed: 1.0